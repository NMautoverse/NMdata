---
title: "Data creation tools"
output:  rmarkdown::html_vignette
Suggests: markdown
VignetteBuilder: knitr
vignette: >
  %\VignetteIndexEntry{Data creation tools}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
header-includes:
- \usepackage{ae}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
 ,fig.width=7)

knitr::opts_chunk$set(tidy.opts=list(width.cutoff=60), tidy=TRUE)
```

```{r,eval=TRUE,include=FALSE}
## library(devtools)
## load_all("C:/Users/delff/working_copies/NMdata")
```

```{r,setup,include=F}
library(NMdata)
library(data.table)
library(ggplot2)
theme_set(theme_bw()+theme(legend.position="bottom"))

```
Built `r Sys.Date()` using NMdata `r packageVersion("NMdata")`.


## Introduction
Nonmem has quite a number of restrictions on the format of the input data, and
problems with the data set is a very common reason for Nonmem not to behave as
expected. When this happens, you easily waste a lot time debugging the
data. `NMdata` includes some simple functions to prevent these situations.

This vignette uses `data.table` syntax for the little bit of data manipulation
performed. However, you don't need to use data.table _at all_ to use these tools
or any tool in `NMdata`. The reason why the functions return data.table objects
is that the data set is a `data.table` to begin with

```{r}
pk <- readRDS(file=system.file("examples/data/xgxr2.rds",package="NMdata"))
class(pk)
```

If this was a `data.frame`, the `NMdata` functions would keep returning objects
of class `data.frame`.


## Automated checks of merge results
Merges are a very common source of data creation bugs. As simple as they may
seem, merges likely leave you with an unexpected number of rows, some repeated
or some omitted. Merges are very powerful and flexible, and when preparing a
data set, they are fundamental. However, many merges for dataset creation are of
the specific and simple kind where columns are added to an existing dataset. In
this situation the rows that come out of the merge are the exact same as in one
of the existing datasets. Say we want to add a covariate from a
`dt.cov`. Imagine we just assume that all subjects are represented with one row
each in `dt.cov`, and so we expect the number of rows to be unchanged from `pk`:

```{r,include=FALSE}
dt.cov <- pk[,.(ID=unique(ID)[1:10])]
dt.cov[,COV:=sample(1:5,size=10,replace=TRUE)]

dt.cov2 <- pk[,.(ID=unique(ID))]
dt.cov2[,COV:=sample(1:5,size=.N,replace=TRUE)]
dt.cov2 <- dt.cov2[c(1,1:(.N-1))]
```

```{r}
dim(pk)
## the number of unique values of ID in pk
pk[,uniqueN(ID)]
pk[,range(ID)]
## dt.cov has a covariate for some of the subjects
dt.cov
pk2 <- merge(pk,dt.cov,by="ID")
dim(pk2)
## now as expected
pk3 <- merge(pk,dt.cov,by="ID",all.x=TRUE)
dim(pk3)
```

So when creating `pk2` we lost a lot of rows because we forgot the
`all.x` option. And also as we shall now see, just checking the
resulting dimensions is not enough.

```{r}
dt.cov2
pk4 <- merge(pk,dt.cov2,by="ID")
dim(pk4)
## we now have twice as many rows for this subject
pk[ID==31,.N]
pk4[ID==31,.N]
```

As illustrated by these simple examples, even the simplest merges have
to be rigorously checked, no exceptions. The function `mergeCheck(x1,x2)`
ensures that the result has exactly one of each rows from `x1` and
nothing else. We can use `...` to pass additional arguments to merge.

```{r}
pk2 <- try(mergeCheck(pk,dt.cov,by="ID"))
## now as expected
pk3 <- mergeCheck(pk,dt.cov,by="ID",all.x=TRUE)
dim(pk3)
```

The result always has the same row order as the `x1` argument.

Another problem that the programmer may not realize during a merge is
that column names are shared across `x1` and `x2` (in addition to columns that are being merged by). This will silently create
colmn names like `col.x` and `col.y` in the output. `mergeCheck` will
by default give a warning if that happens.

There
is only one difference from the behaviour of the `merge.data.frame`
function syntax, being that the `by` argument must always be supplied to `mergeCheck`.

You may think that this will limit your merges, and that you need
merges for inner and outer joins etc. You are exactly right -
`mergeCheck` is not intended for those merges and does not support
them. When that is said, the kind of merges that are supported, are
indeed very common. To illustrate how common, all merges in the
`NMdata` package are performed with `mergeCheck`.


## Exclusion flags
It is good practice not to discard records from a dataset but to flag
them and omit them in model estimation. When reporting the analysis,
we also need to account for how many data records were discarded due
to which criteria. A couple of functons in `NMdata` help you do this
in a way that is easy to integrate with Nonmem. 

The implementation in `NMdata` is based on sequentially checking
exclusion conditions. This means we can summarize how many records and
subjects were excluded from the analysis due to the different
criteria. The information is represented in one numerical column for
Nonmem, and one (equivalent) column for the rest of us in the
resulting data. 

It is good practice to keep all information needed to evaluate the
exclusion criteria in the data. Say we exclude observations if they
are flagged for reanalysis by bioanalysis. If we first run a criteria
for time to be non-negative, we would no longer know which of the
observations with negative time that were flagged for reanalysis based
on the exclusion flag. 


### Assign and count flags
For use in Nonmem, the easiest is that inclusion/exclusion is
determined by a single column in data - we call that column `FLAG`
here, but any column name can be used. `FLAG` obviously draws on
information from other columns such as `TIME`, `DV`, and many others,
depending on your dataset and your way of working.


The function that applies inclusion/excluasion rules is called
`flagsAssign`, and it takes a dataset and a data.frame with rules as
arguments.

```{r}
pk <- readRDS(file=system.file("examples/data/xgxr2.rds", package="NMdata"))
dt.flags <- fread(text="FLAG,flag,condition
10,Negative time,TIME<0
20,Below LLOQ,BLQ==1")

pk <- flagsAssign(pk,dt.flags)
```

Your first question may be why `fread` is used to create a data.table
(like `read.csv` to create a data.frame). The reason is that this way, we
can write the information line by line. If you have ten lines in the
dataset above and do `data.frame(FLAG=...,flag=...,condition=...)` it
gets impossible to read what elements of the three columns that
correspond.

`flagsAssign` applies the conditions sequentially and by increasing
value of `FLAG`. `FLAG=0` means that the observation is included in
the analysis. You can use any expression that can be evaluated within
the data.frame. In this case, `BLQ` has to exist in `pk`.

Actually, the evaluated expressions are slightly different from what
is written in the table above. `flagsAssign` will prepend the
condition that `FLAG==0` in order for the conditions to be
sequential. Again, the omission will be attributed to the first
condition matched.


What rows to omit from a dataset can vary from one analysis to
another. Hence, the aim with the chosen design is that the inclusion
criteria can be changed and applied to overwrite an existing
inclusion/exclusion selection. For another analysis we want to include
the observations below LLOQ, so we create a different exclusion flag
for that one:

```{r}
dt.flags2 <- fread(text="FLAG2,flag2,condition
10,Negative time,TIME<0")

pk <- flagsAssign(pk,dt.flags2,col.flagn="FLAG2",col.flagc="flag2")
```



### Summarize data exclusions
An overview of the number of observations disregarded due to the
different conditions is then obtained using `flagsCount`:

```{r}
tab.count <- flagsCount(data=pk,tab.flags=dt.flags)
print(tab.count)
tab.count2 <- flagsCount(data=pk,tab.flags=dt.flags2,col.flagn="FLAG2",col.flagc="flag2")
print(tab.count2)
```

`flagsCount` includes a `file` option to save the the table right
away.

## Finalize data format and write to file
Once you have your dataset in place, `NMdata` provides a few useful
functions. At this point, ideally you check your data for any
inconsistency. If you miss an inconsistency, you need to wait for Nonmem
to finish if the inconsistency is not fatal for Nonmem's ability to
run the model. Then, you may still not realize and continue working
with Nonmem to build the model before you realize that something is
wrong. `NMdata` is still ways from providing a sufficient set of tests
to run, but still these checks are useful and easy and cheap to run.

### Automated ordering of columns
The order of columns in Nonmem is important for two reasons. One is
that a character in a variable read into Nonmem will make the run
fail. The other is that there are restrictions on the number of
variables you can read into Nonmem, depending on the
version. `NMcolOrders` tries to put the used columns first, and
other or maybe even unusable columns in the back of the dataset. It
does so by a mix of recognition or column names and analysis of the
column contents. 

Columns that cannot be converted to numeric are put in the back, while
column bearing standard Nonmem variable names like `ID`, `TIME`, `EVID` etc.
will be pulled up front. You can of course add column names to
prioritize to front (`first`) or back (`last`). See `?NMorderColumns` for
more options.

```{r}
pk <- NMorderColumns(pk)
```

### Writing data to files
For the final step of writing the dataset, `NMwriteData` is
provided. Most importantly, it writes a csv for Nonmem and an rds for
R with equal contents, but with the rds including all information
(such as factor levels) which cannot be saved in csv (NMscanData will
know how to make use of this information once you retrieve the Nonmem
results). It also provides a proposal for text to include in the
`$INPUT` and `$DATA` sections of the Nonmem control
streams - for you to copy-paste. 

```{r}
NMwriteData(pk)
```

If a file name had been provided, the data would have been written,
and the path to the data file would have been included in the message
written back to the user. There are several arguments that will affect
the proposed text for the Nonmem run, see `?NMwriteData`.




## Stamp objects for traceability
The last couple of functions that will be introduced here are used for
tracing datasets to data creation scripts, including time stamps and
other information you want to include with the data set. 

```{r}
pk <- stampObj(pk,script="vignettes/DataCreate.Rmd")
objInfo(pk)
```
The `script` argument is recognized by `stampObj`, but you can add anything to this. Say you want to keep descriptive note too:

```{r}
pk <- stampObj(pk,script="vignettes/DataCreate.Rmd",Description="A PK dataset used for examples.")
objInfo(pk)
```

These are very simple functions. But they are simple to use as well,
and hopefully they will help you avoid sitting with a data set trying
to guess which script generated it so you can do a modification or
understand how something was done.

When using `NMwriteData`, you don't have to call `stampObj`
explicitly. Just pass the `script` argument to `NMwriteData` and
`stampObj` will be applied automatically.


## Subject-level variables
In a proper PK/PD analysis we need to explore the data at multiple
variability levels. What we looked at above is at dosing and sampling
level (one row per dosing or sampling event). `NMdata` provides very
useful functions to extract information at other levels of
variability. Before extracting the subject-level information we will
add an individual exposure measure to the dataset. We will use the
empirical Bayes' estimate of the individual maximum
concentration. This is derived as the maximum prediction across the
sample times - it may be better to simulate the model at a richer time
scale to get better precision. Again, we make use of `data.table` but
feel free to use what you prefer. `findCovs` - like other `NMdata`
functions - will work on `data.frame`'s, `tibble`'s and other
`data.frame`-like classes 
(classes that inherit from `data.frame`).

```{r}
res1 <- NMscanData(system.file("examples/nonmem/xgxr001.lst", package="NMdata"),
                   col.row="ROW",quiet=TRUE,as.fun="data.table")
res1$trtact <- reorder(res1$trtact,res1$DOSE)
## with data.table, create a new column representing ID-level Cmax
res1[,Cmax:=max(IPRED),by=.(ID)]
## findCovs picks the columns that do not vary within cols.id. One row
## per value of cols.id.
res1.id <- findCovs(res1,cols.id="ID")
dim(res1.id)
ggplot(res1.id,aes(WEIGHTB,Cmax/DOSE,colour=trtact))+
    geom_point()+
    labs(x="Bodyweight at baseline (kg)")
```

`cols.id` can be longer than one. Include a study column, a treatment column to
support cross-over, or a model
column if you are comparing model fits (see below how to do this very easily).

If your model includes occasion variability, you probably also want to
look at 

```{r}
## we have no occasion variability in this data
## res1.id.occ <- findCovs(res1,cols.id=c("ID","OCC"))
```

The simplest use of `findCovs` is to get variables that are constant
across the whole dataset:


```{r}
findCovs(res1)
```

Let's take a look at what is in the
`res1.id` generated above. It is a mix of variables that vary at subject level and variables
that are constant across the full dataset. 

```{r}
dim(res1.id)
head(res1.id,2)
```


`findCovs` has a counterpart in `findVars` which finds variables that
do vary within constant values of optional `cols.id` columns. To get only the ones that
vary within the dataset (i.e. they are truely subject-level
variables and not say a study number if we only have one study), we can do 

```{r}
res1.id2 <- findVars(res1.id)
dim(res1.id2)
head(res1.id2,2)
```

`findVars` supports the cols.id argument too. So you can use
`findVars(res1,cols.id="ID")` to find variables that are non-constant
within (at least one value of) ID. `cols.id` can be of arbitrary length.

Of course, we most often know what covariates or other subject-level
variables to look at, and we would not search for them after running a
nonmem model. But in the situation where you are looking at someone
else's work or you are doing a meta analysis across models where the
data has been coded slightly differently, these simple tools can be
very useful. Or if you like writing generic wrapper functions, you may
find this very handy. Also, remember that if a variable is returned by
these functions, you know that they fullfill the variability
requirement. We know that variables in `res1.id` above do not vary
within `ID`, meaning that no `ID` has more than one value of the
variable. `NA` counts as any other value, so if a value is returned
for subject in `res1.id`, this subject does not have `NA`'s in `res1`.
