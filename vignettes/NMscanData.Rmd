---
title: "NMscanData"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{get-started}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r,setup}
library(NMdata)
```

# Introduction
Getting data from R to Nonmem and back can be tedious and time
consuming. The way data is exported from Nonmem means that a few
tables may have to be combined, and then some variables may still be
missing (like character variables which may be in the input data
file). Most modellers develop some habits over time to avoid some
issues. But still, it may be time demanding even for experienced
modellers to pick up a model developed by someone else or by
themselves earlier in their career just because they need to
understand what data exported and how. This vignette focuses on how
NMdata provides a very general solution for what needs to be trivial:
get a dataset out of a Nonmem run. And general means that it will work
out of the box 
in as many cases as possible. It will do its best to understand how
the data can be read and combined, (ideally) regardless of the way the
model was written.

Try NMscanData on any dataset:
```{r,eval=TRUE}
res1 <- NMscanData(NMdata_filepath("examples/nonmem/xgxr002.lst"))
head(res1)
```

These four returned datasets represent the following variability levels:
- population (model run) level (like study name or TVCL representing clearance for a typical subject)
- subject-level like covariates, individual parameter estimates or between-subject variable random effects.
- occasion-level like between-occasion variable random effects or parameters
- row-level which is everything that varies within the levels above. That is time, concentrations, doses, time-varying covariates etc.

In future versions of NMdata, only the row-level dataset will be returned. Let's see how to derive other levels of variability from the returned data.


# Get started
All you have to do is point to a lst file (or however you name the output file):

```{r,eval=FALSE}
res1 <- NMscanData(NMdata_filepath("examples/nonmem/run001.lst"))
names(res1)
lapply(res1,dim)
```

The lst file was scanned for output tables, and they were all read
(including interpreting the possible firstonly option). The input data
has been used based on the DATA and INPUT sections of the control
stream. Then it analyses what varies withing the levels mentioned in
the introduction. Variables that do not vary at all are included at
pop level, those that do not vary within (any) subject ID, are put at
id-level etc. Everything is included at row level.

A little bit of information is needed to do this, and that is namely
what variables that represent the different levels. NMscanData will
assume that row counter is called ROW, subject identifier ID, and
occasion identifier OCC. They can all be changed by arguments. Also,
even combinations of columns can be used to represent the levels
above. Use col.occ=c('PART','OCC') if OCC is nested in study parts
etc.

## The row identifier
The row identifier is not needed for the function to run. If not
existing, it does however mean that NMscanData cannot make use of the
input data file. If you want to be able to use input data, always make
sure to have a row identifier in your dataset, and make sure to export
it in at least one table in nonmem. Future versions of this package
may include features to try to make use of the input file anyway, but
it will never as good as if you include a row counter in both input
and output data. If you do not have one in an old model run, consider
if a combination of columns can make it up for one, like
c("ID","EVID","TIME"). No general solution exist here, you need to
analyse the data to find out.

## How the input data is read

