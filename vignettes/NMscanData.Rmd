---
title: "NMscanData"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{get-started}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
,fig.width=7)
```

```{r,setup}
## .libPaths("~/R/x86_64-pc-linux-gnu-library/3.5/")
## library(devtools)
## load_all("~/working_copies/NMdata")

library(NMdata)

library(ggplot2)
```

# Introduction
Getting data from R to Nonmem and back can be tedious and time
consuming. The way data is exported from Nonmem means that a few
tables may have to be combined, and then some variables may still be
missing (like character variables which may be in the input data
file). Most modellers develop some habits over time to avoid some
issues. But still, it may be time demanding even for experienced
modellers to pick up a model developed by someone else or by
themselves earlier in their career just because they need to
understand what data exported and how. This vignette focuses on how
NMdata provides a very general solution for what needs to be trivial:
get a dataset out of a Nonmem run. And "general" means that it will
work out of the box in as many cases as possible. It will do its best
to understand how the data can be read and combined, (ideally)
regardless of the way the model was written.

In brevity, the most important steps are

- Read and combine output tables
- If wanted, read input data and restore variables that were not
  output from the nonmem model
- If wanted, also restore rows from input data that were disregarded
  in Nonmem (e.g. observations or subjects that are not part of the
  analysis).
  
It should not be to hard to do. But with the large degree of
flexibility Nonmem offer, there are a lot of caveats to be aware of,
again especially if the model wasn't written by your own (current)
standards.

# Get started
Try NMscanData on any model:
```{r,eval=TRUE}
res1 <- NMscanData(NMdata_filepath("examples/nonmem/xgxr001.lst"))
class(res1)
```
NMscanData says that it found both input and output data and has sent
a data.table back. The `NMdata` class allows for keeping additional
information in the object, but notice that it is also a data.table. If
you are not used to data.table and don't want to use them, feel free
to convert to another class, or use the as.dt=FALSE argument and get a
data.frame back. NMdata uses data.table under the hood but all
functions will work even if you don't. In the future, NMdata may be
able to return tibbles or other classes as well.

Let's have a quick look at the data we got back. The following is
again done with data.table but the comments in the code should make it
clear what happens. 

The data used for the example is a PK single ascending dose data set
borrowed from the xgxr package, kindly allowed by the xgxr team.

```{r,eval=TRUE}
## trtact is a character. Make it a factor with levels ordered by numericaldose level.
res1[,trtact:=reorder(trtact,DOSE)]
## Derive another data.table with geometric mean pop predictions by treatment and nominal sample time. Only use sample records.
res1.mean <- res1[EVID==0,.(gmPRED=exp(mean(log(PRED)))),by=.(trtact,NOMTIME)]
## plot individual observations and geometric mean pop predictions. Split by treatment.
ggplot(res1[EVID==0])+
geom_point(aes(TIME,DV))+
## stat_summary(aes(x=NOMTIME,y=PRED),fun.y=function(x)exp(mean(log(x))),geom="line")+
geom_line(aes(NOMTIME,gmPRED),data=res1.mean)+
scale_y_log10()+
facet_wrap(~trtact,scales="free_y")+
labs(x="Hours since administration",y="Concentration (ng/mL)")

```

You see from the plot that the obtained dataset contains both model
predictions (i.e. from output tables) and a character variable,
`trtact` (i.e. from input data). NMscanData has read both and combined
them. 

## Subject-level variables
In a proper PK/PD analysis we need to explore the data at multiple
variability levels. What we looked at above is at dosing and sampling
level. NMdata provides very useful functions to extract information at
other levels of variability. Before extracting the subject-level
information we will add an individual exposure measure to the
dataset. We will use the emperical Bayes' estimate of the individual
maximum concentration. This is derived as the maximum prediction
across the sample times - it may be better to simulate the model at a
richer time scale to get better precision.

```{r}
res1[,Cmax:=max(IPRED),by=.(ID)]
res1.id <- findCovs(res1,cols.id="ID")
dim(res1.id)
ggplot(res1.id,aes(WEIGHTB,Cmax/DOSE,colour=trtact))+
geom_point()
```

If your model includes occasion variability, you probably also want to
look at 

```{r}
## we have no occasion variability in this data
## res1.id.occ <- findCovs(res1,cols.id=c("ID","OCC"))
```

Let's use the same function to see the variables that are constant
across the whole dataset


```{r}
findCovs(res1)
```

`findCovs` has a counterpart in `findVars` which finds variables that
do vary within optional columns. Let's take a look at what is in the
`res1.id` generated above.

```{r}
dim(res1.id)
head(res1.id,2)
```

This is a mix of variables that vary at subject level and varibles
that are constant across the full dataset. To get only the ones that
vary within the dataset (i.e. they are truely de facto subject level
variables), we can do 

```{r}
res1.id2 <- findVars(res1.id,cols.id="model")
dim(res1.id2)
head(res1.id2,2)
```

Of course, we most often know what covariates or other subject-level
variables to look at, and we would not search for them after running a
nonmem model. But in the situation where you are looking at someone
else's work or you are doing a meta analysis across models where the
data has been coded slightly differently, these tools indeed come in
useful.

By the way, the variable called `model` above is a column created by
NMscanData. You can specify both column name and content as arguments
in NMscanData, but if you don't it will do it this way, and take the
model name from the lst file name. In fact, we could have left out the
cols.id argument in that findVars call because there is only one model
in the data (only difference would be that `model` wouldn't be in the
resulting data). But now you know what the model column is.

# More options and features
## Recover rows
You may have wondered why there is so little data on the small doses
in the plot of observations and geometric mean population predictions
above. The reason is that observations below the lower limit of
quantification have been disregarded. For this and many other reasons,
it is very common to use `ACCEPT` and `IGNORE` statements in the
`$INPUT` section of nonmem control streams. NMscanData can include
this data in the returned data object as well. Let's redo the plot
above taking advantage of this option:

```{r}
res1 <- NMscanData(NMdata_filepath("examples/nonmem/xgxr001.lst"),recoverRows=TRUE)
res1[,trtact:=reorder(trtact,DOSE)]
## Derive another data.table with geometric mean pop predictions by treatment and nominal sample time. Only use sample records.
res1.mean <- res1[EVID==0&nmout==TRUE,.(gmPRED=exp(mean(log(PRED)))),by=.(trtact,NOMTIME)]
## plot individual observations and geometric mean pop predictions. Split by treatment.
ggplot(res1[EVID==0])+
geom_point(aes(TIME,DV,colour=flag))+
## stat_summary(aes(x=NOMTIME,y=PRED),fun.y=function(x)exp(mean(log(x))),geom="line")+
geom_line(aes(NOMTIME,gmPRED),data=res1.mean)+
scale_y_log10()+
facet_wrap(~trtact,scales="free_y")+
labs(x="Hours since administration",y="Concentration (ng/mL)")
```

Notice, one thing changed when calculating the population geometric
means by nominal time and dose. We included `nmout==TRUE`. `nmout` is
a boolean column created by NMscanData expressing whether the rows
were in the output data (`nmout==TRUE`) or they were recovered from
the input data. `PRED` is obviously not calculated for data rows
neglected by Nonmem, so we want to keep those out of the
calculation. Let's see how many were recovered 
```{r}
## this is just a long-format representation of
## with(res1,table(nmout,flag)) using data.table.
res1[,.N,by=.(nmout,flag)]
```
So 905 were part of the analysis. 

## The row identifier or interpret Nonmem code
The row identifier is not needed for NMscanData to run and in most
cases succesfully and correctly complete all the steps above. However,
the most robust way is to include a unique row identifier in both
input and at least one full-length output table.

## Preserve all input data properties
In the code above when plotting the population predictions together
with individual observations above, we saw that character variables
(the treatment as dose with unit) was read from the input data file
and was used for splitting the plots. However, in order to sort the
plots naturally by increasing dose level, we had to run `reorder` on
the variable. This is because the input data is read from a csv
file. NMscanData will not code any character variables as factor when
reading from text files.

Since the character representation of treatment was already prepared,
it would be natural to encode the factor levels already at that
point. In order to preserve such information, we can use R's native
rds format. If the argument useRDS is TRUE, NMscanData will look for
an rds file next to the input data file (which is a delimited text
file) the exact same name as the text file except the extension must
be .rds rather than say ".csv" (for nonmem and NMscanData, the
extension of the delimited text file doesn't matter). If it finds the
rds file, this will be used instead. No checks are done of whether the
contents are similar in any way to the delimited text file which is
ignored in this case. 



# What should I do for my models to be compatible with NMscanData
The answer to this should be as close to "nothing" as possible -
that's more or less the aim of the function. You just have to make
sure that the information that you need is present in input data and
output data. No need to output information that is unchanged from
input, but make sure to output what you need (like IPRED, CWRES, CL,
ETA1 etc which cannot be found in input). Some of these values can be
found from other files generated by Nonmem but notice: NMscanData uses only
input and output data.

It is recommended to always use a unique row identifier in both input and
output data. This is the most robust way to merge back with input
data. In firstonly tables, include the subject ID. Again, everything
will most likely work even if you don't, I personally don't like
relying on "most likely" when I can just as well have robustness.

# What exactly will NMscanData return?
So far, the merge has been very straightforward, but in many
	situations, choices have to be made. Imagine the following scenarios:
- A variable is present in both input and an output table. Values
      differ.
- Two output tables return the same variable but in different resolution.
- A variable from input data is output in a FIRSTONLY table. It varies
  within some subjects in the input data.
  
The following main principles are followed
- Output data prevails over input data
- Row-specific output data is preferred over firstonly tables
- If rows are being recovered from input data, no information from
  output is not merged onto these rows.
- The primary aim is to return the output data. If input and output
  cannot be meaningfully combined (very rare), output will be
  returned.
  

# The building blocks
The lst file was scanned for output tables, and they were all read
(including interpreting the possible firstonly option). The input data
has been used based on the DATA and INPUT sections of the control
stream. 


## Read an output table
NMreadTab

## Read all output data

## Input data means the data as used by Nonmem
You may already have thought about this for a while reading this
vignette. Nonmem doesn't read the variable or column names from the
input dataset. Names are assigned in the control stream. Moreover,
some columns may be dropped, and rows can be filtered using IGNORE and
ACCEPT conditions. NMdata has functions to interpret this, and to read
a dataset as interpreted by a nonmem model, point NMtransInput to the
model.

NMreadCsv

NMtransInput



# Limitations

input file must exist and not have been modified since model run.

Some filters

Assumes psn-like file names (lst and mod).
