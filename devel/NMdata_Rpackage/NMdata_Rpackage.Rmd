---
title: |
  NMdata: A fast R package for efficient data preparation, consistency-checking and post-processing in PK/PD modeling
author: Philip Delff
date: April, 2021
fontsize: 8pt
output: 
 beamer_presentation:
  slide_level: 2
  keep_tex: true
classoption: "aspectratio=169"
---

<!-- TODO: -->
<!-- Motivation -->
<!-- Describe testing framework -->
<!-- Pretty up -->
<!-- Tools for programming -->
<!--  - egdt -->
<!--  - findCovs -->
<!--  - findVars -->
<!--  - NMreadSection -->
<!--  - NMextractText -->

```{r,include=FALSE}

library(knitr)
opts_chunk$set(fig.path='figure/beamer-',fig.align='center',fig.show='hold',size='footnotesize')

library(ggplot2)
theme_set(theme_bw(base_size=14)+
          theme(legend.position="bottom")+
          theme(strip.background =element_rect(fill="#ff9dff"))
          )
library(data.table)
unloadNamespace("NMdata")

                                        # some setup
options(width=60)  # make the printing fit on the page
set.seed(1121)   # make the results repeatable
```

<!-- Supposedly this chunk option should activate a profile but it doesn't work -->
<!-- ,class.source="smaller" -->
```{css, echo=FALSE}
.watch-out {
  background-color: lightpink;
  border: 3px solid red;
  font-weight: bold;
}
.smaller {
  background-color: lightgreen;
  font-size: 4pt;
}

```

## Outline
\tableofcontents[hideallsubsections]


# Introduction

## What is NMdata?
::: columns
:::: column
### NMdata is 

An R package that can help

*  Creating event-based data sets for PK/PD modeling
*  Keeping Nonmem code updated to match contents of datasets
*  Read all output data and combine with input data from Nonmem runs
   - supply output list file (.lst), and the reader is very flexible and automated 

Designed to fit in to the user's setup and coding preferences

* NMdata comes with a configuration tool that can be used to tailor default behaviour to the user's system configuration and preferences.

::::

:::: column

### NMdata is not

* A plotting package

* A tool to retrieve details about model runs

* A calculation or simulation toolbox
* A "silo" that requires you to do things in a certain way.
  - No tools in NMdata requires other NMdata tools to be used

::::
:::

$$\vspace{.01in}$$

The data creation tools should be relevant independently of estimation/simulation tool.

<!-- ## Who can find NMdata useful? -->

<!-- * The data set creation tools are relevant no matter the estimation and simulations tools. -->

<!-- * Nonmem users will find additional tools for handling the exchange of data between R and Nonmem. -->


<!-- ## About the author -->
<!-- * Pharmacometrician with experience from biostatistics -->

<!-- * Background in engineering, experience as system administrator, 15 years of R experience -->

<!-- * Very concerned with code robustness and ensuring data consistency. -->

<!-- * Authored an R package on safe data transfer from SAS to R and one on survival analysis.  -->

<!-- I hate being stuck in leg work and having too little time for modeling, -->
<!-- reflection, and understanding key questions. `NMdata` is a big help for -->
<!-- me personally in freeing time to more high-level tasks. -->


<!-- Lots of work missing on this one -->
<!-- ## Motivation -->

<!-- PK/PD modeling is technically extremely heavy. We want to do provide clarity to decision making, but spend a lot of our time in deep mud. -->

<!-- `NMdata` is my humble experience collected in efficient functions that fill some holes and help with some of the most annoying design -->
## Motivation
* As large a potential pharmacometrics has for illuminating the unknown in drug development, it is dangerously technical.

* I hate being stuck in leg work and having too little time for modeling,
reflection, and understanding key questions. `NMdata` is a big help for
me personally in freeing time to more high-level tasks.

* During the first 2-3 years I spent in pharmacometrics, I must have spent half the time coding, desparately trying to get Nonmem to behave and to understand the properties of the estimates I obtained.

* Most of us develop our own ways to avoid some of the many difficulties in this proces. 

* Due to change of job and therapeutic area, mine had to be refined. 

* Being a fairly experienced R programmer, I generalized some of these methods and collected them in `NMdata`.

* I have no intention of missioning these approaches to others. But if
you find something interesting, feel free to take advantage.


<!-- This could become a good slide, but so far not ready at all -->
<!-- ## Overview of NMdata functionality -->
<!-- * Data creation -->
<!-- - Checking of compatibility of data.frames. -->
<!-- - Merge with automated checks  -->

<!-- * Nonmem control stream editing -->

<!-- * Retrieve data from Nonmem -->



# Getting started
## Getting started

```{r,install-NMdata,message=FALSE}
library(remotes)
install_github("philipdelff/NMdata",upgrade="never")
```

```{r,load-NMdata,echo=TRUE,message=TRUE}
library(NMdata)
```

```{r,load-all-NMdata,include=FALSE}
library(devtools)
load_all()
```

```{r,include=FALSE}
NMdataConf(check.time=FALSE)
NMdataConf(as.fun="data.table")
```


Three vignettes are available so far (see "Vignettes" tab when visiting URL above):

* [Data creation tools](https://philipdelff.github.io/NMdata/articles/DataCreate.html)
* [Automated and general reader of Nonmem data](https://philipdelff.github.io/NMdata/articles/NMscanData.html)
* [FAQ](https://philipdelff.github.io/NMdata/articles/NMdata-FAQ.html)

<!-- dplyr users, please see FAQ for how to make all NMdata functions return tibbles. -->

For a quick overview (after installation), do:

<!-- Would be good to show a truncated output here -->
```{r,eval=F}
help(package="NMdata")
```

All functions and their arguments are documented in their help files. 

<!-- prepare example data behind the scenes -->
```{r,prep-data,include=FALSE}
pk <- readRDS(file=system.file("examples/data/xgxr2.rds",package="NMdata"))
pk[,trtact:=NULL]
## will create this in the example
pk[,ROW:=NULL]

pk.reduced <- copy(pk)
pk.reduced <- pk.reduced[1:(.N%/%2)]
pk.reduced[,CYCLE:=NULL]
pk.reduced[,AMT:=as.character(AMT)]
```

# Data set creation

## Compare compatibility of data sets for rbind and merge
::: columns
:::: column

* In order to rbind or merge data sets, they must be compatible in 
  - presence of columns, depending of desired outcome
  - equally importantly, the classes of the common columns.
* compareCols provides an overview of these properties for any number of data sets. 
  - By default, only descripancies are returned. 
  - Using diff.only=FALSE will give the complete list of columns in the two datasets.

:::: 
:::: column

A slightly modified version of the `pk` dataset has been created.

* `CYCLE` has been removed, and
* `AMT` has been recoded to character

```{r,compareCols,class.source="smaller"}
compareCols(pk,pk.reduced)
```

\vspace{12pt}

Before merging or stacking, we may want to recode `AMT` in one of the datasets to get the class we need, and decide what to do about the missing `CYCLE` in one of the datasets (if OK, values are filled with `NA`).

::::
:::


<!-- %%% renameByContents.R -->
## Rename columns based on contents
::: columns
:::: column 
### renameByContents
* Nonmem almost entirely relies on numeric data values.
* The source data
will often contain character variables, i.e. columns with non-numeric
data values. We want to use these and other non-numerics in post-processing.
* If the column names reflect whether the values are numeric, mistakes and double-checking can be avoided. 
* `renameByContents` renames columns if a function of their contents returns `TRUE`.

### `NMisNumeric`

- `NMisNumeric` is a function that tests if the contents are numeric to `Nonmem`. 
- Subject ID `"1039"` (character class) will be a numeric in Nonmem, `"1-039"` will not. 
- We invert that, and those that Nonmem cannot interpret as numeric become lowercase. 

:::: 

:::: column
\footnotesize
```{r,include=FALSE}
pk.old <- copy(pk)
```

```{r}
pk <- renameByContents(data=pk,
                       fun.test=NMisNumeric,
                       fun.rename = tolower,
                       invert.test = TRUE)
```

`compareCols` shows that four columns were renamed:

```{r}
compareCols(pk.old,pk)
```
\normalsize
::::

:::

<!-- %%% list additional small function -->
<!-- %% dims -->

## Automated checking of merges

* Merges are a very common source of data creation bugs. 
* As simple as they may seem, merges likely leave you with an unexpected number of
rows, some repeated or some omitted. 
* `mergeCheck` is a wrapper of `merge` 
which only accepts the results if 

**The rows that come out
of the merge are the exact same as in one of the existing datasets,
only columns added from the second dataset**

* This limitation of the
scope of the merge allows for a high degree of automated checks of
consistency of the results.

* This is not to say that merges beyond the scope of `mergeCheck` are
relevant or necessary. But if `mergeCheck` covers your needs, it's a
real time saver in terms of automated checks that you are getting
what you expect.

**mergeCheck is not a new implementation of merge. It's an implementation of checks.**

* `mergeCheck` uses `merge.data.table`. The addition is the checks that the result is in accordance with the restrictions. This means

* The order of rows in the resulting data is always the same as the first dataset supplied.

Does that make it slower?

- If you don't use data.table already, `mergeCheck` is likely to be way faster than what you use already. 
- The checking overlay should be neglegible.
- If checks fail, an additional merge is done to help user identify problems. This may cost significant additional time but is likely to save you coding and (at least) the same calculation time anyway.


## mergeCheck
\frametitle{Example: Would your standard checks of merges capture this?}

```{r,include=FALSE}
dt.cov <- pk[,.(ID=unique(ID))]
dt.cov[,COV:=sample(1:5,size=.N,replace=TRUE)]
dt.cov <- dt.cov[c(1,1:(.N-1))]
```

Say we want to add a covariate from a
`dt.cov`.  We expect the number of rows to be unchanged from `pk`. `mergeCheck` requires that we get all and only the _same_ rows:

::: columns
:::: column
### Without `mergeCheck`
\footnotesize
```{r}
## The resulting dimensions are correct
pk4 <- merge(pk,dt.cov,by="ID")
dims(pk,dt.cov,pk4)

## But we now have twice as many rows for this subject
dims(pk[ID==31],pk4[ID==31])
```
:::
:::: column
### `mergeCheck` throws an error
...and suggests what is wrong
\footnotesize

```{r}
try(mergeCheck(pk,dt.cov,by="ID"))
```
::::
\normalsize
:::

### Conclusion
If you only want to add columns by a merge, `mergeCheck` does all the necessary checks for you.

## Exclusion flags
\frametitle{Keep track of data exclusions - don't discard!}

* It is good practice not to discard unwanted records from a dataset but to flag
them and omit them in model estimation. 

* When reporting the analysis,
we need to account for how many data records were discarded due
to which criteria. 

* The implementation in `NMdata` is based on sequentially checking
exclusion conditions.

* The information is represented in one numerical column for
Nonmem, and one (value-to-value corresponding) character column for
the rest of us.

## FlagsAssign
::: columns
:::: column
* `flagsAssign` applies the conditions sequentially and by increasing or decreasing
value of `FLAG`. 

* `FLAG=0` means that none of the conditions were met and row is kept in analysis.  This cannot be changed.

* You can use any expression that can be evaluated _row-wise_ within
the data.frame. In this case, `BLQ` has to exist in `pk`.

* If you need to evaluate a condition based on multiple rows (say inadequate dosing history for a subject), do that first, and include a column representing this condition.

* In `Nonmem`, you can include `IGNORE=(FLAG.NE.0)` in `$DATA` or `$INFILE`.

::::

:::: column
\footnotesize
```{r,include=FALSE}
pk[,`:=`(FLAG=NULL,flag=NULL)]
```

```{r}
dt.flags <- fread(text="FLAG,flag,condition
10,Below LLOQ,EVID==0&BLQ==1
100,Negative time,EVID==0&TIME<0")

pk <- flagsAssign(pk,tab.flags=dt.flags,subset.data="EVID==0")
pk[EVID==1,FLAG:=0]
pk[EVID==1,flag:="Dosing"]
```
::::

:::

## `flagsCount`
* An overview of the number of observations disregarded due to the
different conditions is then obtained using `flagsCount`:

* `flagsCount` includes a `file` argument to save the the table right
away.

```{r,include=F}
opts <- options(width=100)
```

\footnotesize
```{r}
flagsCount(data=pk[EVID==0],tab.flags=dt.flags)
```

```{r,include=F}
options(opts)
```
Now pick the columns you want and format your table for the report.


# Finalize data for Nonmem 
## Advice: always include a unique row identifier

::: columns
:::: column
### Why
A unique identifier is needed in order to

* Track rows in analysis data back to source data

* Reliably combine (by merge) output with input data

### The identifier should be

* Numeric
  - For Nonmem to be able to read it
* Integer
  - To avoid risk of rounding
  - It is _not_ a problem if represented as a `double` in `R`

* Increasing
  - Not strictly necessary
  - Avoid confusion
  - May be useful for post-processing to have a single column to order by

::::
:::: column

### Sort rows and add a row counter with `data.table`
```{r}
setorder(pk,ID,TIME,EVID)
pk[,ROW:=1:.N]
```
::::
:::


<!-- *`NMscanData` will most often work even if you don't include a row -->
<!-- identifier. But this will rely on interpretation of Nonmem code. -->

## NMorderColumns
::: columns
:::: column
\vspace{12pt}
The order of columns in Nonmem is important for two reasons. 

* Character in a variable read into Nonmem will make the run
fail
* The number of
variables you can read into Nonmem is restricted 

Uses a mix of recognition of column names and analysis of the
column contents to sort the columns. `NMorderColumns` does not sort rows, nor does it modify any contents of columns.


* First: Standard columns (`ID`, `TIME`, `EVID` etc.) and usable columns first

* Columns that cannot be converted to numeric are put in the back

* Additional columns to place earlier (argument `first`) or late (`last`) can be specified. 

* See `?NMorderColumns` for more options.

::::
:::: column
<!-- Use print.data.table to get top and bottom with correct row numbers -->
\footnotesize
```{r}
pk.old <- copy(pk)
pk <- NMorderColumns(pk,first="WEIGHTB")
```
\normalsize
We may want to add `MDV` and rerun `NMorderColumns`.
\footnotesize
```{r}
data.table(old=colnames(pk.old),new=colnames(pk))
```
::::
\normalsize
:::

## NMwriteData
::: columns
:::: column

For the final step of writing the dataset, `NMwriteData` is
provided. 

* Checks character variables for Nonmem compatibility (commas not allowed)
* writes a csv file with appropriate options for Nonmem compatibility
* Default is to also write an rds file for R 
  - Contents identical to R object including all information (such as factor levels) which cannot be saved in csv. 
  - If you use `NMscanData` to read Nonmem results,  this information can be used automatically. 

* Provides a proposal for text to include in the
`$INPUT` and `$DATA` sections of the Nonmem control
streams. 

### The csv writer is very simple
These are the only steps involved between the supplied data set and the written csv.

* `scipen` is small to maximize precision.

\footnotesize
```{r,eval=FALSE}
file.csv <- fnExtension(file,".csv")
fwrite(data,na=".",quote=FALSE,row.names=FALSE,scipen=0,file=file.csv)
```
\normalsize 

All arguments to `fwrite` can be modified using the `args.fwrite` argument.

::::
:::: column
\footnotesize
```{r}
NMwriteData(pk,file="derived/pk.csv")
```
\normalsize 

\vspace{12pt}

* `NMwriteData` _Never_ modifies the data.

* `eff0` is the last column in `pk` that `Nonmem` can make use of (remember `NMisNumeric` from earlier?)

* `NMwriteData` detected the exclusion flag and suggests to include it in `$DATA`.

::::
:::


## Update Nonmem control streams
::: columns
:::: column

* `NMwriteSection` is a function that replaces sections (like $DATA or
  $TABLE) of nonmem control streams.

* `NMwriteData` returns a list that can be directly processed by
  `NMwriteSection`

* In `NMwriteData`, several arguments modify the proposed text
the proposed text for the Nonmem run, see `?NMwriteData`.

### Tips
* `NMextractDataFile` takes a control stream/list file and extracts
  the input data file name/path. You can use this to identify the
  model runs in which to update `$DATA`.

* `NMwriteData` is very useful for many other sections, like `$TABLE`,
  or even `$PK`. But not `$THETA` and `$OMEAGE` (because they are
  specific to each model).

* `NMwriteData` by defaults saves a backup of the overwritten control
  streams.

* `NMwriteData` has a counterpart in `NMreadSection`

::::
:::: column

\footnotesize
```{r}
nmCode <- NMwriteData(pk,file="derived/pk.csv",
                      write.csv=FALSE,
### arguments that tailors text for Nonmem
                      nmdir.data="../derived",
                      nm.drop="PROFDAY",
                      nm.rename=c(CONC="DV"),
                      ## PSN compatibility
                      capitalize.names=TRUE)
```

```{r,eval=FALSE}
## example: pick run1*.mod
models <- list.files("../models",pattern="run1.+\\.mod$",
                     full.names=T)
## update $INPUT and $DATA
lapply(models,NMwriteSection,list.sections=nmCode)
## update $INPUT 
lapply(models,
       NMwriteSection,section="INPUT",newlines=nmCode$INPUT)
```

\normalsize

::::
:::


## Automated documentation of data
If the argument `script` is
supplied to `NMwriteData`, a little meta information is attached to
the saved `rds` object.

\footnotesize

```{r}
NMwriteData(pk,file="derived/pk.csv",script = "NMdata_Rpackage.Rmd")
pknm <- readRDS("derived/pk.rds")
objInfo(pknm)
```

\normalsize

`NMwriteData` uses `stampObj` which you can use on any R object to attach similar meta information. Additional arguments (essentially anything) can be passed to `stampObj` using the argument `args.stamp`.

`stampObj` and `objInfo` write and read an "attribute" called `objInfo`.

THE SAME INFORMATION IS WRITTEN TO PK_META.TXT WHEN PK.CSV IS WRITTEN

# Configuration of NMdata defaults
## NMdataConf
::: columns

:::: column

* `NMdataConf` supports changing many default argument values, simplifying coding. 

* Notice, values are reset when `library(NMdata)` or `NMdataConf(reset=TRUE)` are called. 

* See all currently used values by `NMdataConf()`.

::::

:::: column
My initialization of scripts often contain this: 
```{r,eval=FALSE}
library(NMdata)
NMdataConf(as.fun="data.table"
### this is the default value
          ,col.row="ROW"
### Recommended but _right now_ not default
          ,merge.by.row=TRUE
### You can switch this when script is final
          ,quiet=FALSE)
```

::::
:::
Other commonly used settings in `NMdataConf` are

- `as.fun`: a function to apply to all objects before returning them from `NMdata` functions. If you use `dplyr/tidyverse`, do (notice, no quotes!):
```{r,eval=FALSE}
library(tibble)
NMdataConf(as.fun=tibble::as_tibble)
```

- `recover.rows`: Should `NMscanData` Include rows not processed by Nonmem? (default `FALSE`).

- `use.input`: Should `NMscanData` combine (output data) with input data? (default `TRUE`)

- `file.mod`: A function that translates the list file path to the input control stream file path. Default is to replace extension with `.mod`.

- `check.time`: Default is `TRUE`, meaning that output (list file and tables) are expected to newer than input (control stream and input data). If say you copy files between systems, this check may not make sense.

## Why not use `options()`?
`R` has a system for handling settings. `NMdata` does not use that.

* Main reason: `NMdataConf` can check both setting/argument names and values for consistency. 
```{r}
try(NMdataConf(asfun=tibble::as_tibble))
try(NMdataConf(use.input="FALSE"))
```

* A few extra features are available with `NMdataConf`:
  - Reset all settings: `NMdataConf(reset=TRUE)` 
  - Reset individual settings: `NMdataConf(use.input=NULL, as.fun=NULL)`
  - Retrieve all current settings: `NMdataConf()`


# Retrieving data from Nonmem runs 

## NMscanData
`NMscanData` is an automated and general reader of Nonmem data. It returns one combined data set.

- Read and combine output tables
- If wanted, read input data and restore variables that were not
  output from the `Nonmem` model
- If wanted, also restore rows from input data that were disregarded
  in `Nonmem` (e.g. observations or subjects that are not part of the
  analysis)
- Multiple checks for consistency 

\pause
\footnotesize
```{r}
```
::: columns
:::: column
```{r}
file1.lst <- system.file("examples/nonmem/xgxr003.lst",
                         package="NMdata")
res0 <- NMscanData(file1.lst,merge.by.row=FALSE)
```
::::
\pause
:::: column
```{r}
class(res0)
dims(res0)
head(res0,n=2)
```
\normalsize
::::
:::

## Remember the unique row identifier
<!-- Recommend unique row identifier -->
Using a unique row identifier for merging data is highly recommended:

\footnotesize
```{r}
file1.lst <- system.file("examples/nonmem/xgxr001.lst", package="NMdata")
res1 <- NMscanData(file1.lst,merge.by.row=TRUE)
class(res0)
```
\normalsize


## NMscanData
\framesubtitle{Example: quickly get from a list file to looking at the model}

<!-- ```{r,include=FALSE} -->
<!-- ## trtact is a character. Make it a factor with levels ordered by -->
<!-- ## numerical dose level. -->
<!-- # res1$trtact <- reorder(res1$trtact,res1$DOSE) -->
<!-- ``` -->

\footnotesize
:::::::::::::: {.columns}
::: {.column width="45%"}
<!-- ::: columns -->
<!-- :::: column -->
```{r,aplot,eval=FALSE}
## Using data.table for easy summarize
res1 <- NMscanData(file1.lst,merge.by.row=TRUE,
                   as.fun="data.table",quiet=TRUE)
## Derive geometric mean pop predictions by
## treatment and nominal sample time. Only
## use sample records.
res1.mean <-
    res1[EVID==0,
            .(gmPRED=exp(mean(log(PRED)))),
            by=.(trtact,NOMTIME)]
## plot individual observations and geometric
## mean pop predictions. Split (facet) by treatment.
ggplot(subset(res1,EVID==0))+
    geom_point(aes(TIME,DV))+
    geom_line(aes(NOMTIME,gmPRED),
              data=res1.mean,colour="red")+
    scale_y_log10()+
    facet_wrap(~trtact,scales="free_y",ncol=2)+
    labs(x="Hours since administration",
         y="Concentration (ng/mL)")
```
<!-- :::: -->
<!-- :::: column -->

:::
::: {.column width="55%"}

\normalsize

```{r,aplot,echo=FALSE,eval=TRUE,out.width="105%"}
```

<!-- :::: -->
<!-- ::: -->

:::
::::::::::::::

## Recover discarded rows

:::::::::::::: {.columns}
::: {.column width="45%"}

```{r,include=FALSE}
NMdataConf(as.fun="data.table")
system.file("examples/nonmem/xgxr014.lst", package="NMdata")
```

\footnotesize
```{r,meanbydose}
res2 <- NMscanData(file1.lst,
                   merge.by.row=TRUE,recover.rows=TRUE)
```
:::
::: {.column width="55%"}

```{r,echo=FALSE,out.width="105%"}
## Derive another data.table with geometric mean pop predictions by
## treatment and nominal sample time. Only use sample records.
res2.mean <- res2[EVID==0&nmout==TRUE,
                  .(gmPRED=exp(mean(log(PRED)))),
                  by=.(trtact,NOMTIME)]
## plot individual observations and geometric mean pop
## predictions. Split by treatment.
ggplot(res2[EVID==0])+
    geom_point(aes(TIME,DV,colour=flag))+
    geom_line(aes(NOMTIME,gmPRED),data=res2.mean)+
    scale_y_log10()+
    facet_wrap(~trtact,scales="free_y",ncol=2)+
    labs(x="Hours since administration",y="Concentration (ng/mL)")
```
:::
::::::::::::::

## Compare models
\framesubtitle{Example: Renaming and combining models by `rbind`}

:::::::::::::: {.columns}
::: {.column width="45%"}
\footnotesize
```{r}
NMdataConf(as.fun="data.table")
NMdataConf(col.row="ROW")
NMdataConf(merge.by.row=TRUE)
```

```{r,compmodels,eval=F}
## notice fill is an option to rbind with data.table
lst.1 <- system.file("examples/nonmem/xgxr001.lst",
                     package="NMdata")
lst.2 <- system.file("examples/nonmem/xgxr014.lst",
                     package="NMdata")
res1.m <- NMscanData(lst.1,quiet=TRUE)
res2.m <- NMscanData(lst.2,quiet=TRUE,
                     modelname="single-compartment")

res.mult <- rbind(res1.m,res2.m,fill=T)
res.mult.mean <- res.mult[EVID==0&nmout==TRUE,
                          .(gmPRED=exp(mean(log(PRED)))),
                          by=.(model,trtact,NOMTIME)]

ggplot(res.mult.mean,aes(NOMTIME,gmPRED,colour=model))+
    geom_line()+
    scale_y_log10()+
    geom_point(aes(TIME,DV),data=res1.m,
               alpha=.5,colour="grey")+
    labs(x="Hours since administration",y="Concentration (ng/mL)")+
    facet_wrap(~trtact,scales="free_y",ncol=2)
```

:::
::: {.column width="55%"}
\normalsize
```{r,compmodels,eval=T,echo=FALSE,out.width="105%",warning=FALSE}
```

:::
::::::::::::::


## Preserve all input data properties
::: columns
:::: column
By default, `NMscanData` will look for an rds file next to the csv file (same file name, only extension .rds different). 

* If this is found, it will be read, providing an enriched (e.g. conserving factor levels and any other information).

* There are no checks of consistency of `rds` file against delimited file read by `Nonmem`.
 - I am interested in ideas on how to do this. If we can avoid reading the csv file, it would be highly prefered.

* You get the rds automatically if using `NMwriteData`.
 
* Disable looking for the rds by argument `use.rds=FALSE`. 

* Default value of `use.rds` can be modified with `NMdataConf`.

::::
:::: column
Notice, the plots are correctly ordered by doses - because they are ordered as a factor.
```{r}
lst <- system.file("examples/nonmem/xgxr014.lst",
                   package="NMdata")
res14 <- NMscanData(lst,quiet=TRUE)
```

```{r,echo=FALSE,out.width="105%"}
## Derive another data.table with geometric mean pop predictions by
## treatment and nominal sample time. Only use sample records.
res.mean <- res14[EVID==0&nmout==TRUE,
                  .(gmPRED=exp(mean(log(PRED)))),
                  by=.(trtact,NOMTIME)]
## plot individual observations and geometric mean pop
## predictions. Split by treatment.
ggplot(res14[EVID==0])+
    geom_point(aes(TIME,DV,colour=flag))+
    geom_line(aes(NOMTIME,gmPRED),data=res.mean)+
    scale_y_log10()+
    facet_wrap(~trtact,scales="free_y",ncol=2)+
    labs(x="Hours since administration",y="Concentration (ng/mL)")

```
::::
:::

## The NMdata class
Most important message: an `NMdata` object can be used as if it weren't. `R` looks sequentially for "methods" matching the classes of an object.
```{r}
class(res1)
```
Methods defined for `NMdata`:

* `summary`: The information that is written to the console if `quiet=FALSE`.

This is the only behavior that is overwritten by the `NMdata` class. 

Simple other methods like `rbind` and similar are defined by dropping the `NMdata` class and then perform the operation.

`NMinfo` only works on `NMdata` objects.


## What should I do for my models to be compatible with NMscanData?
* The answer to this should be as close to "nothing" as possible -
that's more or less the aim of the function. 

* (As always) you just have to make
sure that the information that you need is present in input data and
output data. 

* No need to output information that is unchanged from
input, but make sure to output what you need (like `IPRED`, `CWRES`, `CL`,
`ETA1` etc which cannot be found in input). Always output the row identifier!

* Some of these values can be
found from other files generated by `Nonmem` but notice: `NMscanData` uses
only input and output data.

* Including a unique row identifier in both input and
output data is the most robust way to combine the tables. 

* In `firstonly` tables, include the subject ID or the row identifier. 

* Everything
will most likely work even if you don't 
  - I would not take "most likely" when robustness is available.


## Limitations
Even if limitations of NMscanData may be several, they are all rare. There is a very good chance you will never run into any of them. 

* If merging with input data, the input data must be available as was
when the model was run. 
  - Option 1: "Freeze" model runs together with data. `NMfreezeModels` does that and will be included in `NMdata` after a little more testing.
  - Option 2 (platform-dependent): `Nonmem` can be run in a wrapper script that either copies the input
data, or runs `NMscanData` and saves the output in a compressed file
format (like `rds`). 

* Not all data filter statements implemented. Nested `ACCEPT` and `IGNORE` statements are not supported at this
point. The resulting number of rows after applying filters is checked
against row-level output table dimensions (if any available).  It is always recommended to use a unique row identifier in
both input and output tables in order to avoid relying on
interpretation of `Nonmem` code.

* The `RECORDS` and `NULL` options in `$DATA` are not implemented. If using
`RECORDS`, please use the `col.row` option to merge by a unique row
identifier.

* Character time variables not interpreted. If you need this, we can implement it relatively easily.

* Only output tables returning either all rows or one row per
subject can be merged with input. Tables written with options like
`FIRSTLASTONLY` (two rows per subject) and `OBSONLY` are disregarded
with a warning (you can read them with `NMscanTables`). `LASTONLY` is
treated like `FIRSTONLY`, i.e. as ID-level information if not
available elsewhere.


## Data read building blocks

`NMscanData` uses a few simpler functions to read all the data it can find. These functions may be useful when you don't want the full automatic package provided by `NMscanData`.

* `NMreadTab`
  - Fast read and format output tables from Nonmem. 
  - Handles the "`TABLE NO.`" counter
* `NMscanTables` (uses `NMreadTab`)
  - Given a control stream or list file, read all output tables
* `NMreadCsv` 
  - Fast read delimited (input data) files 
* `NMscanInput` (uses `NMreadCSV`)
  - Given a control stream or list file, read input data.
  - Optionally reads and applies Nonmem ignore/accept statements
  - Optionally translates column names according to names used in Nonmem

<!-- # Data processing -->
<!-- ## findCovs, findVars -->

## How is `NMdata` qualified?

`NMdata` includes >60 "unit tests" where results of function calls
with different datasets and arguments are compared to expected
results.

Tests are consistently run before any release of the package.

The tests are crucial in making sure that fixing one bug or
introducing a new feature does not introduce new bugs.

If you have a specific example you want to make sure is tested in the
package, we will include the test in the package.

## Todo
* The next milestone is submitting the package to CRAN
  - Aiming for end of June
* Abstract submitted to ACoP
* The following would be great help in making NMdata more accessible and useful
  - Testing - please use the package and provide feedback
  - Review of documentation, vignettes, and descriptions/explanations on website
  - Graphical representations and illustrations. A hexagon is needed!
  - A tidyverse workflow for a new vignette
* Or if you want to provide code... 
  - A function to test data for concistency and compatibility with Nonmem
  - Improved overview of results from NMscanData
  - ?
<!-- %%%% automatic saving of meta data with csv files -->
<!-- %%%% check that row identifier is not modified by Nonmem -->
<!-- %%%% documentation of columnns -->

# Summary



# Other tools
## pmxtricks
A more diverse package 

- `ggIndProfs`: Individual plots, including indication of doses
- `ggwrite`: Saves images in sizes made for powerpoint, including stamps (time, source, output filename). It can save multiple plots at once as one file (pdf) or multiple files. 


## NMfreezeModels

## "secure" model reader


